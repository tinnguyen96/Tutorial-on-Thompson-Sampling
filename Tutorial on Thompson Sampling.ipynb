{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bandit setting \n",
    "- Multi-armed bandit setting is a formalism of the exploration/exploitation idea. \n",
    "    - K \"arms\", each with an underlying reward $r_k$. Each round of the game, the learner selects an arm and observe the (noisy) reward of the action. Roughly speaking, the goal is to quickly determine the arm with highest reward. \n",
    "    - ad placements is a canonical example: An advertiser has a collection of ads but they don't know the click-through rates i.e. if this ad is shown, what's the probability that the user clicks on it. To find the ad that is most attractive, can formulate as a bandit problem. \n",
    "  \n",
    "  \n",
    "- Comments:\n",
    "    - a form of gradient-free optimization\n",
    "    - the simplest formulation of reinforcement learning (without state transitions) \n",
    "   \n",
    "   \n",
    "- Solutions: Under conditions on the noisy rewards and if regret is the metric by which strategies are measured then these algorithms are great!\n",
    "    - Upper Confidence Bound algorithms: The more rounds any given arm is played the better the (frequentist) confidence interval around the reward of that arm; explore until confident enough, then exploit.  \n",
    "    - Thompson Sampling: Take a Bayesian approach by defining prior over unknown rewards; the more times any arm is played, the posterior contracts to the true bias. Exploration is achieved by sampling rewards from their posterior distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling for Bernoulli bandit\n",
    "- Bernoulli bandit: K arms, each has a bias $r_k \\in [0,1]$ - the reward in each round is $\\text{Ber}(r_k)$. Ad placements can be formulated as a Bernoulli bandit problem. \n",
    "\n",
    "- The algorithm (page 15 of Tutorial) \n",
    "![Thompson Sampling for Bernoulli bandits](bernoulliTS.png /"Thompson Sampling for Bernoulli bandits\")\n",
    "\n",
    "    + Contraction behavior of Beta distribution: for large $\\alpha$ and $\\beta$, $\\text{Beta}(\\alpha, \\beta)$ has sharp peak around the mean $\\frac{\\alpha}{\\alpha + \\beta}$ since the variance goes to $0$. \n",
    "\n",
    "- Visualization https://learnforeverlearn.com/bandits/\n",
    "![visualize](visualize_ts_bernoulli_bandits.png /"Thompson Sampling for Bernoulli bandits\")\n",
    "\n",
    "- Guarantees (Theorem 2 of Regret bound): The expected regret after T rounds is dominated by the logarithm of T (the constants depend on the differences between biases). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thompson Sampling for more general bandit problems \n",
    "Bernoulli bandits is arguably the cleanest formulation for theoretical analysis. Practical situations, you know, are almost never that idealized:  \n",
    "- in addition to being stochastic, rewards might be context-dependent\n",
    "- instead of being failure/successes, rewards might be real-valued\n",
    "\n",
    "Hence practical situations require more theoretical work. Plus computational issues: the nice conjugacy between Beta and Bernoulli breaks down and one needs approximate method to update/sample the posteriors. \n",
    "\n",
    "The heuristics remain the same a) sample using posterior to pick an arm to play and b) update posterior over the played arm's reward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalized news recommendation: a bandit problem with context\n",
    "There is a collection of news articles that one can show on a website; each time a viewer visit a website, we would like to present an article that the viewer is likely to click on. Unlike the Bernoulli bandit setting, we have contextual information on the viewers - so there is likely not a unique news article that will be clicked on by everyone but instead, for different people there is likely a different optimal article. \n",
    "\n",
    "Formulate as a bandit problem: the articles are the arms, the rewards themselves are functions of the context. If we assume that the functions are logistic i.e. all arms have a weight vector which induces the success/failures. We can apply Thompson Sampling to perform Bayesian updates of the weight vectors. \n",
    "\n",
    "![Thompson Sampling for Bernoulli bandits](Personalize_news_rec.png /"Comparable performance of TS and UCB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future directions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "- Tutorial: https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf; \n",
    "- Code to replicate figures in Tutorial: https://github.com/iosband/ts_tutorial\n",
    "- Visualization: https://learnforeverlearn.com/bandits/\n",
    "- Regret bound: http://proceedings.mlr.press/v23/agrawal12/agrawal12.pdf\n",
    "- Applications: https://papers.nips.cc/paper/4321-an-empirical-evaluation-of-thompson-sampling.pdf "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
